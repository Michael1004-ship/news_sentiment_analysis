# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import requests
import json
import os
import sys
import datetime
import pandas as pd
import matplotlib.pyplot as plt
from google.cloud import language_v1
from transformers import pipeline
import csv
from sentistrength import PySentiStr
import nltk
from nltk.corpus import wordnet
import feedparser
import time
import re
from random import randint
import seaborn as sns  # ì¶”ê°€
import numpy as np
from scipy.stats import zscore
from wordcloud import WordCloud  # ì›Œë“œí´ë¼ìš°ë“œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urlparse

# ì½”ë“œ ì‹œì‘ ë¶€ë¶„ì— ì¶”ê°€
load_dotenv()  # .env íŒŒì¼ ë¡œë“œ

# Google API ì¸ì¦ íŒŒì¼ ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸ í•„ìš”)
GOOGLE_APPLICATION_CREDENTIALS = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", "")
HUGGINGFACE_API_KEY = os.environ.get("HUGGINGFACE_API_KEY", "")

# í•„ìš”í•œ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
try:
    nltk.download("wordnet", quiet=True)
except:
    print("NLTK wordnet ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ - ì˜¤í”„ë¼ì¸ì´ê±°ë‚˜ NLTK ì„¤ì¹˜ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")

# ë‰´ìŠ¤ ì†ŒìŠ¤ ì •ì˜
NEWS_SOURCES = {
    "USA": {
        "The New York Times": {"bias": "liberal"},
        "CNN": {"bias": "liberal"},
        "Fox News": {"bias": "conservative"}
    },
    "UK": {
        "The Guardian": {"bias": "liberal"},
        "The Telegraph": {"bias": "conservative"},
        "BBC News": {"bias": "neutral"}
    }
}

# ì¶”ê°€í•  ë‰´ìŠ¤ ì†ŒìŠ¤ ì—…ë°ì´íŠ¸ (main() ì‹¤í–‰ ì „ì— ìœ„ì¹˜ì‹œí‚´)
NEWS_SOURCES["USA"].update({
    "The Washington Post": {"bias": "liberal"},
    "The Wall Street Journal": {"bias": "conservative"},
    "Bloomberg": {"bias": "neutral"},
    "Reuters": {"bias": "neutral"}
})

NEWS_SOURCES["UK"].update({
    "The Times": {"bias": "conservative"},
    "The Independent": {"bias": "liberal"},
    "Financial Times": {"bias": "neutral"},
    "Daily Mail": {"bias": "conservative"}
})

# ì£¼ì œë³„ í‚¤ì›Œë“œ ì •ì˜
TOPIC_KEYWORDS = {
    "Politics": ["election fraud", "immigration policy", "climate policy", "gun control", 
                 "voter suppression", "political polarization", "presidential debate", "democracy crisis"],
    "Economy": ["interest rate hike", "inflation crisis", "recession fears", "stock market crash", 
                "crypto regulation", "economic inequality", "unemployment rate", "consumer confidence index"],
    "Society": ["LGBTQ rights", "racial discrimination", "mental health crisis", "police brutality", 
                "cancel culture", "misinformation", "online harassment", "censorship"],
    "International Relations": ["Ukraine Russia war", "China Taiwan tension", "Middle East crisis", "UN sanctions", 
                                "trade war", "diplomatic conflict", "international peace talks", "NATO expansion"]
}

def analyze_topic_sentiments(df):
    """ì£¼ì œë³„ ê°ì • ì ìˆ˜ì™€ ê°ì • ê°•ë„ë¥¼ ê³„ì‚°"""
    topic_results = []

    for topic, keywords in TOPIC_KEYWORDS.items():
        topic_df = df[df['search_keyword'].isin(keywords)]
        
        if topic_df.empty:
            continue
        
        topic_sentiment_avg = topic_df["final_sentiment_score"].mean()
        topic_intensity_avg = topic_df["sentiment_intensity_score"].mean()
        
        topic_results.append({
            "topic": topic,
            "avg_sentiment_score": topic_sentiment_avg,
            "avg_sentiment_intensity": topic_intensity_avg
        })

    return pd.DataFrame(topic_results)

# ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì§€ì •
RESULTS_DIR = "/home/hwangjeongmun691/ì–¸ë¡ ë³„ ê°ì • ë¶„ì„/ê²°ê³¼"

# ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì •
DEBUG_MODE = True

def debug_print(message, data=None, important=False):
    """ë””ë²„ê·¸ ë©”ì‹œì§€ ì¶œë ¥ í•¨ìˆ˜"""
    if DEBUG_MODE:
        if important:
            print("\n" + "="*50)
            print(f"ğŸ” {message}")
            print("="*50)
        else:
            print(f"ğŸ”¹ {message}")
        
        if data is not None:
            if isinstance(data, str) and len(data) > 300:
                print(f"{data[:300]}... (ìƒëµë¨)")
            else:
                print(data)

# ì˜¤ë¥˜ ë©”ì‹œì§€ ì„¤ì •
def setup_error_handling():
    """ì˜¤ë¥˜ ì²˜ë¦¬ ì„¤ì •"""
    import warnings
    warnings.filterwarnings("ignore", category=UserWarning)
    print("ì•Œë¦¼: êµ­ê°€ë³„, ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµë¥¼ ìœ„í•œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

# WordNetì„ í™œìš©í•œ ë™ì˜ì–´ í™•ì¥
def get_synonyms(keyword):
    """ë‹¨ì–´ì˜ ë™ì˜ì–´ ëª©ë¡ ë°˜í™˜"""
    synonyms = set()
    try:
        for syn in wordnet.synsets(keyword):
            for lemma in syn.lemmas():
                synonyms.add(lemma.name().replace('_', ' '))
    except Exception as e:
        debug_print(f"ë™ì˜ì–´ ì°¾ê¸° ì˜¤ë¥˜: {e}")
    return list(synonyms)

# ì†ŒìŠ¤ ë„ë©”ì¸ ê°€ì ¸ì˜¤ê¸°
def get_source_domain(source):
    """ë‰´ìŠ¤ ì†ŒìŠ¤ì˜ ë„ë©”ì¸ ë°˜í™˜"""
    source_domains = {
        'CNN': 'cnn.com',
        'Fox News': 'foxnews.com',
        'The Guardian': 'theguardian.com',
        'The New York Times': 'nytimes.com',
        'BBC News': 'bbc.com,bbc.co.uk',
        'The Telegraph': 'telegraph.co.uk',
        'Reuters': 'reuters.com',
        'CNBC': 'cnbc.com',
        'Bloomberg': 'bloomberg.com'
    }
    
    return source_domains.get(source, '')

# ì†ŒìŠ¤ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸
def is_same_source(found_source, target_source):
    """ì†ŒìŠ¤ ì´ë¦„ì„ ìœ ì—°í•˜ê²Œ ë¹„êµí•˜ì—¬ ì¼ì¹˜ ì—¬ë¶€ë¥¼ ë°˜í™˜"""
    found_lower = found_source.lower().strip()
    target_lower = target_source.lower().strip()

    # ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
    if found_lower == target_lower:
        return True
    
    # ë¶€ë¶„ ë¬¸ìì—´ í¬í•¨ ì—¬ë¶€ í™•ì¸ (CNN â†” CNN International)
    if target_lower in found_lower or found_lower in target_lower:
        return True
    
    # ë„ë©”ì¸ ê¸°ë°˜ ì†ŒìŠ¤ í™•ì¸
    source_domains = {
        "CNN": "cnn.com",
        "Fox News": "foxnews.com",
        "The Guardian": "theguardian.com",
        "The New York Times": "nytimes.com",
        "BBC News": "bbc.com",
        "The Telegraph": "telegraph.co.uk",
        "Reuters": "reuters.com",
        "Bloomberg": "bloomberg.com",
        "The Times": "thetimes.co.uk",
        "The Independent": "independent.co.uk",
        "Financial Times": "ft.com",
        "Daily Mail": "dailymail.co.uk",
        "The Wall Street Journal": "wsj.com",
        "The Washington Post": "washingtonpost.com"
    }

    if target_source in source_domains and source_domains[target_source] in found_lower:
            return True
    
    # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬
    special_cases = {
        "the times": ["the times", "the times uk", "the sunday times"],
        "the new york times": ["nyt", "ny times", "new york times"],
    }
    
    if target_lower in special_cases:
        target_variants = special_cases[target_lower]
        if found_lower in target_variants:
            return True
        # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì˜ˆì™¸ ì²˜ë¦¬
        if target_lower == "the times" and "new york" in found_lower:
            return False
        if target_lower == "the new york times" and found_lower == "the times":
            return False

    return False

# Google News RSSì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰
def get_google_news_rss(keyword, source=None):
    """Google News RSSì—ì„œ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ (ì†ŒìŠ¤ ë§¤ì¹­ ê°œì„ )"""
    query = keyword
    if source:
        query = f"{keyword} site:{get_source_domain(source)}"
    
    query = requests.utils.quote(query)
    url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"
    debug_print(f"Google News RSS ìš”ì²­ URL: {url}")
    
    try:
        feed = feedparser.parse(url)
        articles = []

        for entry in feed.entries[:15]:  # ê¸°ì‚¬ 15ê°œ ê°€ì ¸ì˜¤ê¸°
            title_parts = entry.title.split(" - ")
            entry_source = title_parts[-1].strip() if len(title_parts) > 1 else "Unknown"
            article_url = entry.link

            # ì†ŒìŠ¤ ì´ë¦„ í‘œì¤€í™”
            normalized_source = normalize_source_name(entry_source)
            
            # ì†ŒìŠ¤ í™•ì¸ - ë” ì—„ê²©í•œ ê²€ì‚¬
            if source and not is_same_source(normalized_source, source):
                debug_print(f"ì†ŒìŠ¤ ë¶ˆì¼ì¹˜: '{normalized_source}' â‰  '{source}' - ê±´ë„ˆëœ€")
                continue
                
            article = {
                "title": title_parts[0].strip(),
                "content": entry.description if hasattr(entry, "description") else title_parts[0].strip(),
                "source": normalized_source,  # í‘œì¤€í™”ëœ ì†ŒìŠ¤ ì´ë¦„ ì‚¬ìš©
                "url": article_url,
                "published_at": entry.published if hasattr(entry, "published") else datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')
            }
            articles.append(article)
        
        debug_print(f"'{keyword}' ê²€ìƒ‰ ê²°ê³¼: {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
        return articles
        
    except Exception as e:
        debug_print(f"Google News RSS ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

# í‚¤ì›Œë“œ í™•ì¥ + Google News ê²€ìƒ‰ ê²°í•©
def search_expanded_news(keyword, source=None):
    """í‚¤ì›Œë“œ í™•ì¥ í›„ ë‰´ìŠ¤ ê²€ìƒ‰"""
    debug_print(f"'{keyword}' í™•ì¥ ê²€ìƒ‰ ì‹œì‘ (ì†ŒìŠ¤: {source})", important=True)
    
    # Step 1: ì›ë˜ í‚¤ì›Œë“œë¡œ Google News ê²€ìƒ‰
    initial_news = get_google_news_rss(keyword, source)
    
    if len(initial_news) >= 5:  # ì¶©ë¶„í•œ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
        debug_print(f"ì›ë˜ í‚¤ì›Œë“œë¡œ ì¶©ë¶„í•œ ê²°ê³¼({len(initial_news)}ê°œ)ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return initial_news[:5]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
    
    # Step 2: í‚¤ì›Œë“œ í™•ì¥ì´ í•„ìš”í•œ ê²½ìš°
    debug_print(f"ê²°ê³¼ê°€ ë¶€ì¡±í•˜ì—¬ í‚¤ì›Œë“œ í™•ì¥ì„ ì‹œë„í•©ë‹ˆë‹¤.")
    expanded_keywords = [keyword]  # ì›ë˜ í‚¤ì›Œë“œ í¬í•¨
    
    # ë‹¨ì¼ ë‹¨ì–´ì¸ ê²½ìš° WordNet ë™ì˜ì–´ ì¶”ê°€
    if ' ' not in keyword and len(keyword) > 3:
        synonyms = get_synonyms(keyword)[:3]  # ìƒìœ„ 3ê°œ ë™ì˜ì–´ë§Œ ì‚¬ìš©
        expanded_keywords.extend(synonyms)
    
    # ì¤‘ë³µ ì œê±°
    expanded_keywords = list(set(expanded_keywords))
    debug_print(f"í™•ì¥ëœ í‚¤ì›Œë“œ: {expanded_keywords}")
    
    # Step 3: í™•ì¥ëœ í‚¤ì›Œë“œë¡œ ì¶”ê°€ ê²€ìƒ‰
    all_news = initial_news.copy()  # ì´ˆê¸° ê²°ê³¼ í¬í•¨
    
    for k in expanded_keywords:
        if k == keyword:  # ì›ë˜ í‚¤ì›Œë“œëŠ” ì´ë¯¸ ê²€ìƒ‰í–ˆìœ¼ë¯€ë¡œ ê±´ë„ˆëœ€
            continue
            
        debug_print(f"í™•ì¥ í‚¤ì›Œë“œ '{k}'ë¡œ ê²€ìƒ‰ ì¤‘...")
        additional_news = get_google_news_rss(k, source)
        
        # ì¤‘ë³µ ì œê±°í•˜ë©° ì¶”ê°€
        for article in additional_news:
            # URLë¡œ ì¤‘ë³µ í™•ì¸
            if not any(existing['url'] == article['url'] for existing in all_news):
                all_news.append(article)
    
    debug_print(f"ìµœì¢… ê²€ìƒ‰ ê²°ê³¼: {len(all_news)}ê°œ ê¸°ì‚¬")
    return all_news[:15]  # ë” ë§ì€ ê¸°ì‚¬ ë°˜í™˜

# ê°ì • ë¶„ì„ í•¨ìˆ˜ë“¤

def get_vader_sentiment(text):
    """VADERë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ì ìˆ˜ ë¶„ì„"""
    try:
        sid = SentimentIntensityAnalyzer()
        sentiment_scores = sid.polarity_scores(text)
        # compound ì ìˆ˜ëŠ” -1ì—ì„œ 1 ì‚¬ì´ì˜ ê°’
        compound_score = sentiment_scores['compound']
        print(f"VADER ì›ë³¸ ì ìˆ˜: {compound_score}")
        return compound_score
    except Exception as e:
        print(f"VADER ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return 0

def get_sentistrength_sentiment(text):
    """SentiStrengthë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ì ìˆ˜ ë¶„ì„"""
    try:
        from sentistrength import PySentiStr
        senti = PySentiStr()
        senti.setSentiStrengthPath("/home/hwangjeongmun691/ì–¸ë¡ ë³„ ê°ì • ë¶„ì„/SentiStrength.jar")
        senti.setSentiStrengthLanguageFolderPath("/home/hwangjeongmun691/ì–¸ë¡ ë³„ ê°ì • ë¶„ì„/SentiStrength_Data/")
        
        # dual ë°©ì‹ìœ¼ë¡œ ê¸ì •, ë¶€ì • ì ìˆ˜ ëª¨ë‘ ë°›ê¸°
        result = senti.getSentiment(text, score='dual')
        if isinstance(result, list):
            result = result[0]  # ì²« ë²ˆì§¸ ê²°ê³¼ë§Œ ì‚¬ìš©
        
        # ê²°ê³¼ëŠ” (ê¸ì •ì ìˆ˜, ë¶€ì •ì ìˆ˜) í˜•íƒœì˜ íŠœí”Œ
        return result[0], result[1]
    except Exception as e:
        print(f"SentiStrength ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return 1, -1  # ì¤‘ë¦½ ê°’ ë°˜í™˜

def get_google_sentiment(text):
    """Google Natural Language APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì • ì ìˆ˜ ë¶„ì„"""
    try:
        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        cred_path = "/home/hwangjeongmun691/ì–¸ë¡ ë³„ ê°ì • ë¶„ì„/comparative-sentiment-analysis-c0b363950560.json"
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = cred_path
        
        # ì¸ì¦ íŒŒì¼ í™•ì¸
        if not os.path.exists(cred_path):
            print(f"âš ï¸ ì¸ì¦ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cred_path}")
            return 0
        
        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = language_v1.LanguageServiceClient()
        
        # ë¬¸ì„œ ê°ì²´ ìƒì„±
        document = language_v1.Document(
            content=text,
            type_=language_v1.Document.Type.PLAIN_TEXT
        )
        
        # ê°ì • ë¶„ì„ ìˆ˜í–‰
        sentiment = client.analyze_sentiment(request={'document': document}).document_sentiment
        
        # Google APIëŠ” -1.0 ~ 1.0 ë²”ìœ„ì˜ ì ìˆ˜ë¥¼ ë°˜í™˜
        score = sentiment.score
        print(f"Google API ì›ë³¸ ì ìˆ˜: {score}")
        return score
    except Exception as e:
        print(f"Google ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return 0  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë¦½ê°’ ë°˜í™˜

def get_huggingface_sentiment(text):
    try:
        # ëª¨ë¸ ëª…ì‹œì  ì§€ì •ìœ¼ë¡œ ê²½ê³  ì œê±°
        sentiment_analyzer = pipeline("sentiment-analysis", 
                                    model="distilbert-base-uncased-finetuned-sst-2-english")
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ ë¶„ì„)
        max_length = 512
        truncated_text = text[:max_length] if len(text) > max_length else text
        result = sentiment_analyzer(truncated_text)[0]
        
        # NEGATIVE ê²°ê³¼ì— ëŒ€í•œ ì²˜ë¦¬ ê°œì„ 
        if result['label'] == 'POSITIVE':
            score = result['score']
        else:
            score = 1 - result['score']  # ë” ì§ê´€ì ì¸ ë³€í™˜
            
        print(f"Hugging Face ì ìˆ˜: {score} (ì›ë˜ ë ˆì´ë¸”: {result['label']})")
        return score
    except Exception as e:
        print(f"Hugging Face ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
        return 0.5

# ì ìˆ˜ ì •ê·œí™” í•¨ìˆ˜ë“¤

def normalize_vader_score(score):
    """VADER ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”"""
    normalized_score = (score + 1) / 2
    return normalized_score

def normalize_sentistrength_score(positive_score, negative_score):
    """SentiStrength ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”"""
    combined_score = (positive_score + (6 + negative_score)) / 10
    return combined_score

def normalize_google_score(score):
    """Google API ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”"""
    normalized_score = (score + 1) / 2
    return normalized_score

# ìµœì¢… ê°ì • ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜

def calculate_final_sentiment(vader_score, google_score, huggingface_score, df=None):
    """ê° ë„êµ¬ì˜ ì •ê·œí™”ëœ ê°ì • ì ìˆ˜ë¥¼ ê°€ì¤‘ í‰ê· í•˜ì—¬ ìµœì¢… ê°ì • ì ìˆ˜ ê³„ì‚° (ê·¹ë‹¨ê°’ ì¡°ì • í¬í•¨)"""
    
    # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì„¤ì •
    vader_weight = 0.4
    google_weight = 0.4
    huggingface_weight = 0.2

    # Z-scoreë¥¼ í™œìš©í•˜ì—¬ ê°ì • ë¶„ì„ ë„êµ¬ì˜ ì ìˆ˜ ê·¹ë‹¨ì„± íŒë‹¨
    if df is not None:
        df = calculate_z_scores(df)  # Z-score ê³„ì‚°
        huggingface_zscore = df["huggingface_score_zscore"].iloc[-1]  # ë§ˆì§€ë§‰ ë°ì´í„° ê¸°ì¤€
        vader_zscore = df["vader_score_zscore"].iloc[-1]
        google_zscore = df["google_score_zscore"].iloc[-1]

        # Hugging Face ê°ì • ì ìˆ˜ì˜ ê·¹ë‹¨ì„± í™•ì¸
        if abs(huggingface_zscore) > 2.0:
            huggingface_weight = 0.1
            print(f"Hugging Face ê°ì • ì ìˆ˜ ì¡°ì • (Z-score: {huggingface_zscore:.2f}) â†’ ê°€ì¤‘ì¹˜ {huggingface_weight}")

        # VADER ê°ì • ì ìˆ˜ì˜ ê·¹ë‹¨ì„± í™•ì¸
        if abs(vader_zscore) > 2.0:
            vader_weight = 0.3
            print(f"VADER ê°ì • ì ìˆ˜ ì¡°ì • (Z-score: {vader_zscore:.2f}) â†’ ê°€ì¤‘ì¹˜ {vader_weight}")

        # Google NLP ê°ì • ì ìˆ˜ì˜ ê·¹ë‹¨ì„± í™•ì¸
        if abs(google_zscore) > 2.0:
            google_weight = 0.3
            print(f"Google ê°ì • ì ìˆ˜ ì¡°ì • (Z-score: {google_zscore:.2f}) â†’ ê°€ì¤‘ì¹˜ {google_weight}")

    # ì •ê·œí™”ëœ ê°ì • ì ìˆ˜ ê³„ì‚°
    normalized_vader = normalize_vader_score(vader_score)
    normalized_google = normalize_google_score(google_score)

    # ìµœì¢… ê°ì • ì ìˆ˜ ê³„ì‚°
    final_score = (vader_weight * normalized_vader + 
                   google_weight * normalized_google + 
                   huggingface_weight * huggingface_score)

    return final_score

# ê¸°ì‚¬ ê°ì • ë¶„ì„ í•¨ìˆ˜

def calculate_z_scores(df):
    """ê°ì • ì ìˆ˜ ë° ê°ì • ê°•ë„ë¥¼ Z-scoreë¡œ ë³€í™˜"""
    z_score_df = df.copy()
    
    # ê°ì • ì ìˆ˜ Z-score ì ìš©
    z_score_df["final_sentiment_zscore"] = zscore(df["final_sentiment_score"])
    z_score_df["sentiment_intensity_zscore"] = zscore(df["sentiment_intensity_score"])
    
    # ê°ì • ë¶„ì„ ë„êµ¬ë³„ Z-score ì ìš©
    tools = ["vader_score", "google_score", "huggingface_score"]
    for tool in tools:
        z_score_df[f"{tool}_zscore"] = zscore(df[tool])

    return z_score_df

def analyze_article_sentiment(article, country, search_keyword=None, df=None):
    """ê¸°ì‚¬ ê°ì • ë¶„ì„ ìˆ˜í–‰ ë° Hugging Face ì ìˆ˜ ì¡°ì • í¬í•¨"""
    title = article["title"]
    content = article["content"]
    source = article["source"]
    text = f"{title}. {content}"

    print(f"\nì…ë ¥ ë‰´ìŠ¤ ê¸°ì‚¬: '{title}'")
    print(f"ì¶œì²˜: {source} (êµ­ê°€: {country})")
    print(f"ë‚´ìš© ì¼ë¶€: {content[:100]}...\n")

    # ê°ì • ë¶„ì„ ë„êµ¬ ì ìš©
    vader_score = get_vader_sentiment(text)
    google_score = get_google_sentiment(text)
    huggingface_score = get_huggingface_sentiment(text)
    sentistrength_pos, sentistrength_neg = get_sentistrength_sentiment(text)

    # ê°ì • ì ìˆ˜ ë° ê°ì • ê°•ë„ ê³„ì‚° (Hugging Face ê°€ì¤‘ì¹˜ ì¡°ì • í¬í•¨)
    final_sentiment_score = calculate_final_sentiment(vader_score, google_score, huggingface_score, df)
    sentiment_intensity_score = normalize_sentistrength_score(sentistrength_pos, sentistrength_neg)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n===== ê°ì • ë¶„ì„ ê²°ê³¼ =====")
    print(f"VADER ê°ì • ì ìˆ˜: {vader_score} (ì •ê·œí™”: {normalize_vader_score(vader_score):.4f})")
    print(f"Google API ê°ì • ì ìˆ˜: {google_score} (ì •ê·œí™”: {normalize_google_score(google_score):.4f})")
    print(f"Hugging Face ê°ì • ì ìˆ˜: {huggingface_score:.4f}")
    print(f"ìµœì¢… ê°ì • ì ìˆ˜: {final_sentiment_score:.4f}")
    print(f"SentiStrength ê°ì • ê°•ë„: ê¸ì •={sentistrength_pos}, ë¶€ì •={sentistrength_neg} (ì •ê·œí™”: {sentiment_intensity_score:.4f})")

    return {
        "title": title,
        "source": source,
        "country": country,
        "bias": get_source_bias(source),
        "search_keyword": search_keyword,
        "final_sentiment_score": final_sentiment_score,
        "sentiment_intensity_score": sentiment_intensity_score,
        "vader_score": normalize_vader_score(vader_score),
        "google_score": normalize_google_score(google_score),
        "huggingface_score": huggingface_score,
        "url": article.get("url", ""),
        "published_at": article.get("published_at", datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'))
    }

def get_source_bias(source):
    """ì‹ ë¬¸ì‚¬ì˜ ì •ì¹˜ì  ì„±í–¥ ë°˜í™˜"""
    for country, sources in NEWS_SOURCES.items():
        if source in sources:
            return sources[source]['bias']
    return "unknown"

# êµ­ê°€ë³„, ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµ í•¨ìˆ˜

def compare_news_sources(keywords, articles_per_source=3):
    """ì—¬ëŸ¬ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ í‚¤ì›Œë“œë³„ ê°ì • ë¶„ì„ ë¹„êµ"""
    results = []
    
    # ì›í•˜ëŠ” ë‰´ìŠ¤ ì†ŒìŠ¤ë¡œ ì •í™•íˆ ì œí•œ
    sources = {
        'CNN': 'US',
        'Fox News': 'US',
        'The Guardian': 'UK',
        'The New York Times': 'US',
        'The Telegraph': 'UK',
        'BBC News': 'UK'
    }
    
    for keyword in keywords:
        debug_print(f"\ní‚¤ì›Œë“œ '{keyword}'ì— ëŒ€í•œ ë¶„ì„ ì‹œì‘...", important=True)
        
        for source, country in sources.items():
            debug_print(f"{source}({country})ì—ì„œ '{keyword}' ê²€ìƒ‰ ì¤‘...")
            
            # Google News RSSë¡œ ê¸°ì‚¬ ê²€ìƒ‰
            articles = search_expanded_news(keyword, source)
            
            if not articles:
                debug_print(f"âŒ {source}ì—ì„œ '{keyword}'ì— ëŒ€í•œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ì°¾ì€ ê¸°ì‚¬ ê°œìˆ˜ ì œí•œ
            articles = articles[:articles_per_source]
            
            # ê° ê¸°ì‚¬ ë¶„ì„
            for article in articles:
                # ì¤‘ë³µ API í˜¸ì¶œ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
                time.sleep(randint(1, 3))
                
                result = analyze_article_sentiment(article, country, search_keyword=keyword)
                
                if result:
                    results.append(result)
                    debug_print(f"âœ… ë¶„ì„ ì™„ë£Œ: {result['title']} (ì ìˆ˜: {result['final_sentiment_score']})")
    
    return results

# ê²°ê³¼ ì‹œê°í™” ë° ì €ì¥ í•¨ìˆ˜

def get_date_folder():
    """ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ í´ë”ëª… ìƒì„± (ì˜ˆ: 20240601)"""
    today = datetime.datetime.now()
    return today.strftime("%Y%m%d")

def get_timestamp():
    """í˜„ì¬ ì‹œê°„ì„ íŒŒì¼ëª…ì— ì í•©í•œ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ì˜ˆ: 20240601_143045)"""
    now = datetime.datetime.now()
    return now.strftime("%Y%m%d_%H%M%S")

def get_result_folder():
    """ë‚ ì§œë³„ ê²°ê³¼ í´ë” ê²½ë¡œ ë°˜í™˜ ë° ìƒì„±"""
    date_folder = get_date_folder()
    result_path = os.path.join(RESULTS_DIR, date_folder)
    
    # ê²°ê³¼ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(RESULTS_DIR):
        os.makedirs(RESULTS_DIR)
    
    # ë‚ ì§œë³„ í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(result_path):
        os.makedirs(result_path)
        print(f"ìƒˆ ë‚ ì§œ í´ë” ìƒì„±: {result_path}")
    
    return result_path

def sample_article_review(df, sample_size=5):
    """Sample article review for sentiment analysis validation"""
    print("\nğŸ” Sample Article Sentiment Analysis Review")
    if len(df) < sample_size:
        sample_size = len(df)
        print(f"âš ï¸ Only {sample_size} samples available for review.")
    
    sample_articles = df.sample(sample_size)
    for idx, row in sample_articles.iterrows():
        print(f"\nArticle Title: {row['title']}")
        print(f"Source: {row['source']} (Country: {row['country']})")
        print(f"Search Keyword: {row.get('search_keyword', 'No info')}")
        
        # sentiment ì»¬ëŸ¼ì´ ì—†ìœ¼ë¯€ë¡œ ì ìˆ˜ë§Œ í‘œì‹œí•˜ê±°ë‚˜ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°ì • ìƒíƒœë¥¼ ê³„ì‚°
        sentiment_label = "ê¸ì •ì " if row['final_sentiment_score'] > 0.6 else "ì¤‘ë¦½ì " if row['final_sentiment_score'] > 0.4 else "ë¶€ì •ì "
        print(f"Final Sentiment Score: {row['final_sentiment_score']:.4f} ({sentiment_label})")
        
        # SentiStrengthê°€ ì—†ìœ¼ë©´ ì¶œë ¥ì—ì„œ ì œì™¸
        tool_scores = f"VADER={row['vader_score']:.2f}, Google={row['google_score']:.2f}, HuggingFace={row['huggingface_score']:.2f}"
        if 'sentistrength_score' in row:
            tool_scores = f"VADER={row['vader_score']:.2f}, SentiStrength={row['sentistrength_score']:.2f}, Google={row['google_score']:.2f}, HuggingFace={row['huggingface_score']:.2f}"
        
        print(f"Tool Scores: {tool_scores}")
        print(f"URL: {row.get('url', 'No info')}")
        print("=" * 50)

def visualize_sentiment_by_source(df, result_folder, timestamp):
    """ì‹ ë¬¸ì‚¬ë³„ ê°ì • ì ìˆ˜ ë¶„í¬ ì‹œê°í™”"""
    plt.figure(figsize=(12, 6))
    ax = sns.boxplot(x='source', y='final_sentiment_score', data=df)
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
    plt.title('Sentiment Score Distribution by News Source')
    plt.ylabel('Sentiment Score (0-1)')
    plt.ylim(0, 1)
    plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'source_sentiment_boxplot_{timestamp}.png'))
    print(f"ğŸ“Š ì‹ ë¬¸ì‚¬ë³„ ê°ì • ì ìˆ˜ ë°•ìŠ¤í”Œë¡¯ ì €ì¥ ì™„ë£Œ.")

def compare_sentiment_tools(df, result_folder, timestamp):
    """Compare sentiment scores across different analysis tools"""
    try:
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ í™•ì¸
        available_tools = []
        tool_labels = []
        
        # í•­ìƒ ì¡´ì¬í•˜ëŠ” ê¸°ë³¸ ë„êµ¬ë“¤
        if 'vader_score' in df.columns:
            available_tools.append('vader_score')
            tool_labels.append('VADER')
            
        if 'google_score' in df.columns:
            available_tools.append('google_score')
            tool_labels.append('Google NLP')
            
        if 'huggingface_score' in df.columns:
            available_tools.append('huggingface_score')
            tool_labels.append('HuggingFace')
        
        # SentiStrengthëŠ” sentiment_intensity_scoreë¡œ ì €ì¥ë¨
        if 'sentiment_intensity_score' in df.columns:
            available_tools.append('sentiment_intensity_score')
            tool_labels.append('SentiStrength')
            
        if len(available_tools) < 2:
            print("âš ï¸ ë„êµ¬ ë¹„êµë¥¼ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        # Box plot
        plt.figure(figsize=(10, 6))
        ax = sns.boxplot(data=df[available_tools])
        ax.set_xticklabels(tool_labels)
        plt.title('Sentiment Score Comparison Across Analysis Tools')
        plt.ylabel('Sentiment Score (0-1)')
        plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)
        plt.tight_layout()
        plt.savefig(os.path.join(result_folder, f'tool_comparison_{timestamp}.png'))
        
        # Correlation analysis
        plt.figure(figsize=(10, 8))
        correlation = df[available_tools].corr()
        sns.heatmap(correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1, 
                    xticklabels=tool_labels, yticklabels=tool_labels)
        plt.title('Correlation Between Sentiment Analysis Tools')
        plt.tight_layout()
        plt.savefig(os.path.join(result_folder, f'tool_correlation_{timestamp}.png'))
        print(f"ğŸ“Š ê°ì • ë¶„ì„ ë„êµ¬ ë¹„êµ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ.")
    except Exception as e:
        print(f"âš ï¸ ë„êµ¬ ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def save_topic_statistics(df, result_folder, timestamp):
    """ì£¼ì œë³„ ê°ì • ì ìˆ˜ ë° ê°ì • ê°•ë„ í†µê³„ë¥¼ CSVë¡œ ì €ì¥"""
    topic_stats = []

    # ì „ì²´ ë°ì´í„° í†µê³„ ê³„ì‚°
    overall_stats = {
        "ì „ì²´ ê°ì • ì ìˆ˜ í‰ê· ": df["final_sentiment_score"].mean(),
        "ì „ì²´ ê°ì • ì ìˆ˜ í‘œì¤€í¸ì°¨": df["final_sentiment_score"].std(),
        "ì „ì²´ ê°ì • ì ìˆ˜ ë¶„ì‚°": df["final_sentiment_score"].var(),
        "ì „ì²´ ê°ì • ê°•ë„ í‰ê· ": df["sentiment_intensity_score"].mean(),
        "ì „ì²´ ê°ì • ê°•ë„ í‘œì¤€í¸ì°¨": df["sentiment_intensity_score"].std(),
        "ì „ì²´ ê°ì • ê°•ë„ ë¶„ì‚°": df["sentiment_intensity_score"].var()
    }

    for topic, keywords in TOPIC_KEYWORDS.items():
        topic_df = df[df['search_keyword'].isin(keywords)]
        
        if topic_df.empty:
            continue
        
        topic_sentiment_avg = topic_df["final_sentiment_score"].mean()
        topic_sentiment_std = topic_df["final_sentiment_score"].std()
        topic_intensity_avg = topic_df["sentiment_intensity_score"].mean()
        topic_intensity_std = topic_df["sentiment_intensity_score"].std()
        
        topic_stats.append({
            "topic": topic,
            "avg_sentiment_score": topic_sentiment_avg,
            "std_sentiment_score": topic_sentiment_std,
            "avg_sentiment_intensity": topic_intensity_avg,
            "std_sentiment_intensity": topic_intensity_std
        })

    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    topic_stats_df = pd.DataFrame(topic_stats)

    # CSVë¡œ ì €ì¥
    csv_path = f"{result_folder}/topic_statistics_{timestamp}.csv"
    topic_stats_df.to_csv(csv_path, index=False, encoding="utf-8-sig")

    # ì „ì²´ í†µê³„ë„ í•¨ê»˜ ì €ì¥
    with open(f"{result_folder}/overall_statistics_{timestamp}.csv", "w", newline='', encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["í•­ëª©", "ê°’"])
        for key, value in overall_stats.items():
            writer.writerow([key, value])

    print(f"ğŸ“Š ì£¼ì œë³„ ê°ì • ë¶„ì„ í†µê³„ ì €ì¥ ì™„ë£Œ: {csv_path}")
    print(f"ğŸ“Š ì „ì²´ ê°ì • ë¶„ì„ í†µê³„ ì €ì¥ ì™„ë£Œ: {result_folder}/overall_statistics_{timestamp}.csv")

def plot_topic_sentiments(df, result_folder, timestamp):
    """ì£¼ì œë³„ ê°ì • ì ìˆ˜ì™€ ê°ì • ê°•ë„ë¥¼ ì‹œê°í™”"""
    topic_df = analyze_topic_sentiments(df)

    plt.figure(figsize=(10, 6))
    sns.barplot(data=topic_df, x="topic", y="avg_sentiment_score", palette="coolwarm")
    plt.title("Average Sentiment Score by Topic")
    plt.ylabel("Sentiment Score (0-1)")
    plt.axhline(y=0.5, color="red", linestyle="--", alpha=0.5)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f"topic_sentiment_{timestamp}.png"))
    print(f"ğŸ“Š ì£¼ì œë³„ ê°ì • ì ìˆ˜ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ.")

    plt.figure(figsize=(10, 6))
    sns.barplot(data=topic_df, x="topic", y="avg_sentiment_intensity", palette="viridis")
    plt.title("Average Sentiment Intensity by Topic")
    plt.ylabel("Sentiment Intensity (0-1)")
    plt.axhline(y=0.5, color="red", linestyle="--", alpha=0.5)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f"topic_intensity_{timestamp}.png"))
    print(f"ğŸ“Š ì£¼ì œë³„ ê°ì • ê°•ë„ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ.")

# ê¸°ì¡´ í•¨ìˆ˜ë¥¼ ê°œì„ ëœ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´
def parallel_search_news(keywords, sources, max_threads=8):
    """ë³‘ë ¬ë¡œ ì—¬ëŸ¬ ë‰´ìŠ¤ ê²€ìƒ‰ì„ ìˆ˜í–‰ (ì•ˆì „í•˜ê²Œ ì¤‘ë‹¨ ê°€ëŠ¥)"""
    results = []

    def process(keyword, source, country):
        try:
            articles = search_expanded_news(keyword, source)
            return [analyze_article_sentiment(article, country, search_keyword=keyword) for article in articles[:5]]
        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({keyword}, {source}): {e}")
            return []

    try:
        with ThreadPoolExecutor(max_threads) as executor:
            futures = []
            for keyword in keywords:
                for source, country in sources.items():
                    futures.append(executor.submit(process, keyword, source, country))

            for future in futures:
                try:
                    results.extend(future.result())  # ê²°ê³¼ë¥¼ í•œ ë²ˆì— ì¶”ê°€
                except Exception as e:
                    print(f"âš ï¸ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    except KeyboardInterrupt:
        print("\n\ní”„ë¡œê·¸ë¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì•ˆì „í•˜ê²Œ ì¢…ë£Œí•©ë‹ˆë‹¤...")
        # ì´ë¯¸ ìˆ˜ì§‘ëœ ê²°ê³¼ë§Œ ë°˜í™˜
        return results

    return results

# ë©”ì¸ í•¨ìˆ˜ë¥¼ ê°œì„ ëœ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ êµ­ê°€ë³„ ë° ì‹ ë¬¸ì‚¬ë³„ ê°ì • ê°•ë„ ë¹„êµ ì—°êµ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    try:
        # ì˜¤ë¥˜ ì²˜ë¦¬ ì„¤ì •
        setup_error_handling()

        # ëª¨ë“  í‚¤ì›Œë“œ ì‚¬ìš©
        keywords = [kw for topic in TOPIC_KEYWORDS.values() for kw in topic]
        print(f"ëª¨ë“  {len(keywords)}ê°œ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {keywords}")

        # ëª¨ë“  ì‹ ë¬¸ì‚¬ ì‚¬ìš©
        sources = {}
        # USA ì‹ ë¬¸ì‚¬ ì¶”ê°€
        for source_name in NEWS_SOURCES["USA"]:
            sources[source_name] = 'US'
            
        # UK ì‹ ë¬¸ì‚¬ ì¶”ê°€
        for source_name in NEWS_SOURCES["UK"]:
            sources[source_name] = 'UK'
            
        print(f"ì „ì²´ {len(sources)}ê°œ ì–¸ë¡ ì‚¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤: {list(sources.keys())}")

        # ê¸°ì‚¬ ìˆ˜ì§‘ ë° ë¶„ì„
        results = parallel_search_news(keywords, sources, max_threads=5)  # ìŠ¤ë ˆë“œ ìˆ˜ ê°ì†Œ

        # ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
        if results:
            result_folder = get_result_folder()
            timestamp = get_timestamp()
            
            df = pd.DataFrame(results)
            save_results_to_csv(results)
            save_topic_statistics(df, result_folder, timestamp)  # ê¸°ì¡´ í•¨ìˆ˜
            save_detailed_statistics(df, result_folder, timestamp)  # ìƒˆ í•¨ìˆ˜ ì¶”ê°€
            
            # ì—¬ê¸°ì„œ ì¢…í•© í†µê³„ í•¨ìˆ˜ê°€ ì œëŒ€ë¡œ í˜¸ì¶œë˜ë„ë¡ ìˆ˜ì •
            save_comprehensive_statistics(df, result_folder, timestamp)
            
            # ëª¨ë“  ì‹œê°í™” í•¨ìˆ˜ë¥¼ í™•ì‹¤íˆ í˜¸ì¶œ
            try:
                visualize_sentiment_by_source(df, result_folder, timestamp)
                compare_sentiment_tools(df, result_folder, timestamp)
                plot_topic_sentiments(df, result_folder, timestamp)
                plot_sentiment_over_time(df, result_folder, timestamp) 
                plot_correlation(df, result_folder, timestamp)
                plot_sentiment_distribution(df, result_folder, timestamp)
                plot_sentiment_by_bias_over_time(df, result_folder, timestamp)
                plot_sentiment_by_keyword_over_time(df, result_folder, timestamp)
                generate_wordcloud(df, result_folder, timestamp)
            except Exception as e:
                print(f"ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            print(f"âœ… ì´ {len(results)}ê°œ ê¸°ì‚¬ ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ ì €ì¥ë¨.")
        else:
            print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\n\ní”„ë¡œê·¸ë¨ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def save_results_to_csv(results, filename=None):
    """ë¶„ì„ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    if not results:
        debug_print("ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê²°ê³¼ í´ë” ë° íƒ€ì„ìŠ¤íƒ¬í”„ ì–»ê¸°
    result_folder = get_result_folder()
    timestamp = get_timestamp()
    
    # íŒŒì¼ëª… ì„¤ì •
    if filename is None:
        filename = f"sentiment_analysis_results_{timestamp}.csv"
    
    # ì „ì²´ ê²½ë¡œ ìƒì„±
    filepath = os.path.join(result_folder, filename)
    
    # DataFrameìœ¼ë¡œ ë³€í™˜ í›„ ì €ì¥
    df = pd.DataFrame(results)
    df.to_csv(filepath, index=False, encoding='utf-8-sig')  # UTF-8 with BOM for Excel compatibility
    
    print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
    return filepath

def plot_sentiment_over_time(df, result_folder, timestamp):
    """ì‹œê°„ì— ë”°ë¥¸ ê°ì • ì ìˆ˜ ë³€í™”ë¥¼ ì‹œê°í™”"""
    if not validate_dataframe(df, ['published_at', 'final_sentiment_score'], min_rows=10, purpose="ì‹œê³„ì—´ ë¶„ì„"):
        return
        
    # datetime ë³€í™˜ ë° ì •ë ¬
    df['published_at'] = pd.to_datetime(df['published_at'], errors='coerce')
    df = df.dropna(subset=['published_at', 'final_sentiment_score'])
    df = df.sort_values('published_at')
    
    if len(df) < 10:
        print("âš ï¸ ë³€í™˜ í›„ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì‹œê³„ì—´ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
        
    time_df = df.set_index('published_at')['final_sentiment_score'].resample('W').mean()

    plt.figure(figsize=(12, 6))
    plt.plot(time_df.index, time_df.values, marker='o', linestyle='-')
    plt.title(f'Sentiment Score Over Time (Analysis: {timestamp[:4]}-{timestamp[4:6]}-{timestamp[6:8]})')
    plt.xlabel('Date')
    plt.ylabel('Sentiment Score (0-1)')
    plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.3)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(os.path.join(result_folder, f'sentiment_over_time_{timestamp}.png'))
    print(f"ğŸ“Š ì‹œê°„ì— ë”°ë¥¸ ê°ì • ì ìˆ˜ ë³€í™” ì‹œê°í™” ì™„ë£Œ.")

def plot_correlation(df, result_folder, timestamp):
    """ê°ì • ë¶„ì„ ë„êµ¬ë“¤ ê°„ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ"""
    try:
        # ì‚¬ìš© ê°€ëŠ¥í•œ ê°ì • ë¶„ì„ ë„êµ¬ë“¤ í™•ì¸
        available_tools = []
        tool_labels = []
        
        # ë°ì´í„°í”„ë ˆì„ì— ìˆëŠ” ë„êµ¬ ì»¬ëŸ¼ í™•ì¸
        if 'vader_score' in df.columns:
            available_tools.append('vader_score')
            tool_labels.append('VADER')
        if 'google_score' in df.columns:
            available_tools.append('google_score')
            tool_labels.append('Google NLP')
        if 'huggingface_score' in df.columns:
            available_tools.append('huggingface_score')
            tool_labels.append('HuggingFace')
        if 'sentiment_intensity_score' in df.columns:
            available_tools.append('sentiment_intensity_score')
            tool_labels.append('SentiStrength')
        
        if len(available_tools) < 2:
            print("âš ï¸ ìƒê´€ ê´€ê³„ ë¶„ì„ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        # ìƒê´€ ê´€ê³„ ë¶„ì„
        corr_df = df[available_tools].corr()
        
        # íˆíŠ¸ë§µ ì‹œê°í™”
        plt.figure(figsize=(10, 8))
        sns.heatmap(corr_df, annot=True, cmap="coolwarm", vmin=-1, vmax=1, fmt=".2f",
                    xticklabels=tool_labels, yticklabels=tool_labels)
        plt.title("Correlation Between Sentiment Analysis Tools")
        plt.tight_layout()
        plt.savefig(os.path.join(result_folder, f"sentiment_correlation_{timestamp}.png"))
        print(f"ğŸ“Š ìƒê´€ ê´€ê³„ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ.")
        
    except Exception as e:
        print(f"âš ï¸ ìƒê´€ ê´€ê³„ ë¶„ì„ ì˜¤ë¥˜: {e}")

def generate_wordcloud(df, result_folder, timestamp):
    """ê¸°ì‚¬ ì œëª©ì—ì„œ ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±"""
    try:
        # ëª¨ë“  ê¸°ì‚¬ ì œëª© í•©ì¹˜ê¸°
        all_titles = ' '.join(df['title'].tolist())
        
        # ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±
        wordcloud = WordCloud(width=800, height=400, 
                             background_color='white',
                             max_words=100,
                             contour_width=1,
                             contour_color='steelblue').generate(all_titles)
        
        # ì›Œë“œ í´ë¼ìš°ë“œ ì €ì¥
        plt.figure(figsize=(10, 6))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title('Word Cloud from News Titles')
        plt.tight_layout()
        
        wordcloud_image_path = os.path.join(result_folder, f"wordcloud_{timestamp}.png")
        plt.savefig(wordcloud_image_path)
        
        print(f"ğŸ“Š ì›Œë“œ í´ë¼ìš°ë“œ ì €ì¥ ì™„ë£Œ: {wordcloud_image_path}")
        
        # ê°ì • ì ìˆ˜ì— ë”°ë¥¸ ì›Œë“œ í´ë¼ìš°ë“œ (ê¸ì •/ë¶€ì •)
        # ê¸ì •ì  ê¸°ì‚¬ (ì ìˆ˜ > 0.6)
        positive_df = df[df['final_sentiment_score'] > 0.6]
        if len(positive_df) > 5:  # ìµœì†Œ 5ê°œ ì´ìƒì˜ ê¸°ì‚¬ê°€ ìˆì„ ë•Œë§Œ ìƒì„±
            positive_titles = ' '.join(positive_df['title'].tolist())
            positive_cloud = WordCloud(width=800, height=400, 
                                     background_color='white',
                                     max_words=50,
                                     contour_width=1,
                                     contour_color='green').generate(positive_titles)
            
            plt.figure(figsize=(10, 6))
            plt.imshow(positive_cloud, interpolation='bilinear')
            plt.axis('off')
            plt.title('Word Cloud from Positive News Titles')
            plt.tight_layout()
            plt.savefig(os.path.join(result_folder, f"wordcloud_positive_{timestamp}.png"))
            print(f"ğŸ“Š ê¸ì • ë‰´ìŠ¤ ì›Œë“œ í´ë¼ìš°ë“œ ì €ì¥ ì™„ë£Œ")
        
        # ë¶€ì •ì  ê¸°ì‚¬ (ì ìˆ˜ < 0.4)
        negative_df = df[df['final_sentiment_score'] < 0.4]
        if len(negative_df) > 5:  # ìµœì†Œ 5ê°œ ì´ìƒì˜ ê¸°ì‚¬ê°€ ìˆì„ ë•Œë§Œ ìƒì„±
            negative_titles = ' '.join(negative_df['title'].tolist())
            negative_cloud = WordCloud(width=800, height=400, 
                                     background_color='white',
                                     max_words=50,
                                     contour_width=1,
                                     contour_color='red').generate(negative_titles)
            
            plt.figure(figsize=(10, 6))
            plt.imshow(negative_cloud, interpolation='bilinear')
            plt.axis('off')
            plt.title('Word Cloud from Negative News Titles')
            plt.tight_layout()
            plt.savefig(os.path.join(result_folder, f"wordcloud_negative_{timestamp}.png"))
            print(f"ğŸ“Š ë¶€ì • ë‰´ìŠ¤ ì›Œë“œ í´ë¼ìš°ë“œ ì €ì¥ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âš ï¸ ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± ì˜¤ë¥˜: {e}")

def visualize_results(results):
    """ëª¨ë“  ì‹œê°í™” í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” í†µí•© í•¨ìˆ˜"""
    if not results:
        print("ì‹œê°í™”í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(results)
    
    # ê²°ê³¼ í´ë” ë° íƒ€ì„ìŠ¤íƒ¬í”„ ì–»ê¸°
    result_folder = get_result_folder()
    timestamp = get_timestamp()
    
    # ëª¨ë“  ì‹œê°í™” í•¨ìˆ˜ í˜¸ì¶œ
    try:
        # ê¸°ë³¸ ì‹œê°í™” í•¨ìˆ˜ë“¤
        visualize_sentiment_by_source(df, result_folder, timestamp)
        compare_sentiment_tools(df, result_folder, timestamp)
        plot_topic_sentiments(df, result_folder, timestamp)
        
        # ì‹œê³„ì—´ ê´€ë ¨ ì‹œê°í™” í•¨ìˆ˜ë“¤
        plot_sentiment_over_time(df, result_folder, timestamp)
        plot_sentiment_by_bias_over_time(df, result_folder, timestamp)
        plot_sentiment_by_keyword_over_time(df, result_folder, timestamp)
        
        # ê¸°íƒ€ ì‹œê°í™” í•¨ìˆ˜ë“¤
        plot_correlation(df, result_folder, timestamp)  # ë„êµ¬ ìƒê´€ê´€ê³„ ë¶„ì„ (ì¤‘ë³µ ì œê±°)
        plot_sentiment_distribution(df, result_folder, timestamp)
        generate_wordcloud(df, result_folder, timestamp)
        
        # ìƒ˜í”Œ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
        sample_article_review(df)
        
        # ì¢…í•© í†µê³„ ì €ì¥
        save_comprehensive_statistics(df, result_folder, timestamp)
        
    except Exception as e:
        print(f"ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nâœ… ëª¨ë“  ì‹œê°í™”ì™€ í†µê³„ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ğŸ“ ê²°ê³¼ëŠ” {result_folder} í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def extract_domain(url):
    """URLì—ì„œ ë„ë©”ì¸ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    domain = urlparse(url).netloc
    return domain.replace("www.", "")

def normalize_source_name(source):
    """ì‹ ë¬¸ì‚¬ì˜ ì´ë¦„ì„ í‘œì¤€í™”í•˜ì—¬ ì¼ì¹˜ìœ¨ì„ ë†’ì„"""
    source_mapping = {
        "cnn": "CNN",
        "fox news": "Fox News",
        "the guardian": "The Guardian",
        "nyt": "The New York Times",
        "new york times": "The New York Times",
        "bbc": "BBC News",
        "bbc news": "BBC News",
        "the telegraph": "The Telegraph",
        "reuters": "Reuters",
        "bloomberg": "Bloomberg",
        "the independent": "The Independent",
        "financial times": "Financial Times",
        "ft": "Financial Times",
        "the times": "The Times",
        "daily mail": "Daily Mail",
        "the wall street journal": "The Wall Street Journal",
        "wsj": "The Wall Street Journal",
        "the washington post": "The Washington Post"
    }
    
    source_lower = source.lower()
    for key, value in source_mapping.items():
        if key in source_lower:
            return value
    return source

def plot_sentiment_distribution(df, result_folder, timestamp):
    """ê°ì • ì ìˆ˜ ë¶„í¬ ê·¸ë˜í”„"""
    try:
        plt.figure(figsize=(12, 6))
        sns.histplot(df['final_sentiment_score'], bins=30, kde=True, color='blue')
        plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='ì¤‘ë¦½ì ')
        plt.title('Sentiment Score Distribution')
        plt.xlabel('Sentiment Score (0: Negative, 1: Positive)')
        plt.ylabel('Frequency')
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(result_folder, f"sentiment_distribution_{timestamp}.png"))
        print(f"ğŸ“Š ê°ì • ì ìˆ˜ ë¶„í¬ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ.")
        
        # ì†ŒìŠ¤ë³„ ë¶„í¬ ê·¸ë˜í”„ë„ ì¶”ê°€
        plt.figure(figsize=(14, 8))
        sns.violinplot(x='source', y='final_sentiment_score', data=df, palette='Set3')
        plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.7)
        plt.title('Sentiment Score Distribution by News Source')
        plt.xlabel('News Source')
        plt.ylabel('Sentiment Score')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(os.path.join(result_folder, f"sentiment_distribution_by_source_{timestamp}.png"))
        print(f"ğŸ“Š ì‹ ë¬¸ì‚¬ë³„ ê°ì • ì ìˆ˜ ë¶„í¬ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ.")
        
    except Exception as e:
        print(f"âš ï¸ ë¶„í¬ ê·¸ë˜í”„ ì˜¤ë¥˜: {e}")

def plot_sentiment_by_bias_over_time(df, result_folder, timestamp):
    """ì •ì¹˜ ì„±í–¥ë³„ ì‹œê°„ì— ë”°ë¥¸ ê°ì • ì ìˆ˜ ë³€í™”ë¥¼ ì‹œê°í™”"""
    try:
        if 'published_at' not in df.columns or 'bias' not in df.columns:
            print("âš ï¸ ì •ì¹˜ ì„±í–¥ë³„ ì‹œê³„ì—´ ë¶„ì„ì— í•„ìš”í•œ ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        # datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        if df['published_at'].dtype == 'object':
            df['published_at'] = pd.to_datetime(df['published_at'], errors='coerce')
        
        # ê²°ì¸¡ì¹˜ ì œê±° ë° ì •ë ¬
        df_clean = df.dropna(subset=['published_at', 'final_sentiment_score', 'bias'])
        df_clean = df_clean.sort_values('published_at')
        
        if len(df_clean) < 10:
            print("âš ï¸ ì •ì¹˜ ì„±í–¥ë³„ ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return
        
        # ìœ íš¨í•œ ì •ì¹˜ ì„±í–¥ í™•ì¸
        biases = df_clean['bias'].unique()
        
        # ê·¸ë˜í”„ ìƒì„±
        plt.figure(figsize=(14, 8))
        
        # 1. ì •ì¹˜ ì„±í–¥ë³„ ì‹œê³„ì—´
        plt.subplot(2, 1, 1)
        
        for bias in biases:
            bias_df = df_clean[df_clean['bias'] == bias]
            if len(bias_df) >= 5:
                bias_time_df = bias_df.set_index('published_at')['final_sentiment_score'].resample('W').mean()
                plt.plot(bias_time_df.index, bias_time_df.values, 
                       marker='o', linestyle='-', label=f'{bias.capitalize()}')
        
        plt.title('Sentiment Score Over Time by Political Bias')
        plt.xlabel('Date')
        plt.ylabel('Sentiment Score (0-1)')
        plt.axhline(y=0.5, color='green', linestyle='--', alpha=0.3, label='Neutral Line')
        plt.legend()
        plt.xticks(rotation=45)
        
        # 2. ì •ì¹˜ ì„±í–¥ë³„ ì´ë™ í‰ê·  (ì¶”ì„¸ì„ )
        plt.subplot(2, 1, 2)
        
        for bias in biases:
            bias_df = df_clean[df_clean['bias'] == bias]
            if len(bias_df) >= 10:  # ì´ë™ í‰ê· ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°
                bias_time_df = bias_df.set_index('published_at')['final_sentiment_score']
                # 3ì¼ ì´ë™ í‰ê· 
                plt.plot(bias_time_df.index, bias_time_df.rolling('3D').mean(), 
                       linestyle='-', label=f'{bias.capitalize()} (3-Day MA)')
        
        plt.title('Sentiment Trend by Political Bias (3-Day Moving Average)')
        plt.xlabel('Date')
        plt.ylabel('Sentiment Score (0-1)')
        plt.axhline(y=0.5, color='green', linestyle='--', alpha=0.3)
        plt.legend()
        plt.xticks(rotation=45)
        
        plt.tight_layout()
        plt.savefig(os.path.join(result_folder, f'sentiment_by_bias_over_time_{timestamp}.png'))
        print(f"ğŸ“Š ì •ì¹˜ ì„±í–¥ë³„ ì‹œê°„ì— ë”°ë¥¸ ê°ì • ì ìˆ˜ ì‹œê°í™” ì™„ë£Œ.")
        
        # 3. ì£¼ì œì™€ ì •ì¹˜ ì„±í–¥ì˜ êµì°¨ ë¶„ì„ (íˆíŠ¸ë§µ)
        if 'topic' in df_clean.columns or 'search_keyword' in df_clean.columns:
            # ì£¼ì œ ì»¬ëŸ¼ ì—†ëŠ” ê²½ìš° í‚¤ì›Œë“œì—ì„œ ì£¼ì œ ì¶”ë¡ 
            if 'topic' not in df_clean.columns and 'search_keyword' in df_clean.columns:
                keyword_to_topic = {}
                for topic, keywords in TOPIC_KEYWORDS.items():
                    for keyword in keywords:
                        keyword_to_topic[keyword] = topic
                df_clean['topic'] = df_clean['search_keyword'].map(keyword_to_topic)
            
            pivot_df = df_clean.pivot_table(
                index='topic', 
                columns='bias', 
                values='final_sentiment_score',
                aggfunc='mean'
            ).dropna()
            
            if not pivot_df.empty and pivot_df.shape[0] >= 2 and pivot_df.shape[1] >= 2:
                plt.figure(figsize=(10, 8))
                sns.heatmap(pivot_df, annot=True, cmap='coolwarm', center=0.5, vmin=0, vmax=1)
                plt.title('Average Sentiment Score by Topic and Political Bias')
                plt.tight_layout()
                plt.savefig(os.path.join(result_folder, f'topic_bias_heatmap_{timestamp}.png'))
                print(f"ğŸ“Š ì£¼ì œ-ì •ì¹˜ì„±í–¥ êµì°¨ íˆíŠ¸ë§µ ì €ì¥ ì™„ë£Œ.")
        
    except Exception as e:
        print(f"âš ï¸ ì •ì¹˜ ì„±í–¥ë³„ ì‹œê³„ì—´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

def plot_sentiment_by_keyword_over_time(df, result_folder, timestamp):
    """í‚¤ì›Œë“œë³„ ì‹œê°„ì— ë”°ë¥¸ ê°ì • ì ìˆ˜ ë³€í™”ë¥¼ ì‹œê°í™”"""
    try:
        if 'published_at' not in df.columns or 'search_keyword' not in df.columns:
            print("âš ï¸ í‚¤ì›Œë“œë³„ ì‹œê³„ì—´ ë¶„ì„ì— í•„ìš”í•œ ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        # datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        if df['published_at'].dtype == 'object':
            df['published_at'] = pd.to_datetime(df['published_at'], errors='coerce')
        
        # ê²°ì¸¡ì¹˜ ì œê±° ë° ì •ë ¬
        df_clean = df.dropna(subset=['published_at', 'final_sentiment_score', 'search_keyword'])
        df_clean = df_clean.sort_values('published_at')
        
        if len(df_clean) < 10:
            print("âš ï¸ í‚¤ì›Œë“œë³„ ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return
        
        # í‚¤ì›Œë“œ ì¹´ìš´íŠ¸
        keyword_counts = df_clean['search_keyword'].value_counts()
        
        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” í‚¤ì›Œë“œë§Œ ì„ íƒ (ìµœì†Œ 5ê°œ ì´ìƒ)
        top_keywords = keyword_counts[keyword_counts >= 5].index.tolist()
        
        if len(top_keywords) < 2:
            print("âš ï¸ í‚¤ì›Œë“œë³„ ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•œ ì¶©ë¶„í•œ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì£¼ì œë³„ë¡œ í‚¤ì›Œë“œ ê·¸ë£¹í™”
        keyword_to_topic = {}
        for topic, keywords in TOPIC_KEYWORDS.items():
            for keyword in keywords:
                keyword_to_topic[keyword] = topic
        
        # ê° ì£¼ì œë³„ë¡œ ì‹œê³„ì—´ ê·¸ë˜í”„ ìƒì„±
        topics = set(keyword_to_topic[k] for k in top_keywords if k in keyword_to_topic)
        
        for topic in topics:
            # í•´ë‹¹ ì£¼ì œì˜ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
            topic_keywords = [k for k in top_keywords if k in keyword_to_topic and keyword_to_topic[k] == topic]
            
            if len(topic_keywords) < 2:
                continue
                
            plt.figure(figsize=(14, 8))
            
            for keyword in topic_keywords:
                keyword_df = df_clean[df_clean['search_keyword'] == keyword]
                if len(keyword_df) >= 5:
                    # ì£¼ê°„ í‰ê· ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§
                    keyword_time_df = keyword_df.set_index('published_at')['final_sentiment_score'].resample('W').mean()
                    plt.plot(keyword_time_df.index, keyword_time_df.values, 
                           marker='o', linestyle='-', label=f'{keyword}')
            
            plt.title(f'Sentiment Score Over Time for {topic} Keywords')
            plt.xlabel('Date')
            plt.ylabel('Sentiment Score (0-1)')
            plt.axhline(y=0.5, color='green', linestyle='--', alpha=0.3, label='Neutral Line')
            plt.legend()
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(os.path.join(result_folder, f'sentiment_by_{topic}_keywords_{timestamp}.png'))
            print(f"ğŸ“Š {topic} í‚¤ì›Œë“œë³„ ì‹œê°„ì— ë”°ë¥¸ ê°ì • ì ìˆ˜ ì‹œê°í™” ì™„ë£Œ.")
        
        # í‚¤ì›Œë“œë³„ ê°ì • ì ìˆ˜ í‰ê·  ë¹„êµ (ë§‰ëŒ€ ê·¸ë˜í”„)
        plt.figure(figsize=(16, 8))
        
        # ìƒìœ„ 15ê°œ í‚¤ì›Œë“œë§Œ í‘œì‹œ
        top_n = min(15, len(top_keywords))
        top_n_keywords = top_keywords[:top_n]
        
        # ê° í‚¤ì›Œë“œì˜ í‰ê·  ê°ì • ì ìˆ˜ ê³„ì‚°
        keyword_avg_scores = []
        for keyword in top_n_keywords:
            avg_score = df_clean[df_clean['search_keyword'] == keyword]['final_sentiment_score'].mean()
            keyword_avg_scores.append((keyword, avg_score))
        
        # ê°ì • ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
        keyword_avg_scores.sort(key=lambda x: x[1], reverse=True)
        
        # ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±
        keywords = [k[0] for k in keyword_avg_scores]
        scores = [k[1] for k in keyword_avg_scores]
        
        bars = plt.bar(keywords, scores, color='skyblue')
        plt.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Neutral Line')
        
        # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
        for bar in bars:
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                   f'{height:.2f}', ha='center', va='bottom')
        
        plt.title('Average Sentiment Score by Keyword')
        plt.xlabel('Keyword')
        plt.ylabel('Sentiment Score (0-1)')
        plt.ylim(0, 1)
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(os.path.join(result_folder, f'keyword_avg_sentiment_{timestamp}.png'))
        print(f"ğŸ“Š í‚¤ì›Œë“œë³„ í‰ê·  ê°ì • ì ìˆ˜ ë¹„êµ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ.")
        
    except Exception as e:
        print(f"âš ï¸ í‚¤ì›Œë“œë³„ ì‹œê³„ì—´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

def save_comprehensive_statistics(df, result_folder, timestamp):
    """ëª¨ë“  ì°¨ì›(ì£¼ì œ/êµ­ê°€/ì„±í–¥/ì‹œê°„)ì˜ í†µê³„ë¥¼ CSVë¡œ ì €ì¥"""
    try:
        if df.empty:
            print("âš ï¸ í†µê³„ ì €ì¥ì„ ìœ„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        # 1. ì‹œê°„ë³„ í†µê³„ ì €ì¥
        if 'published_at' in df.columns:
            # datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if df['published_at'].dtype == 'object':
                df['published_at'] = pd.to_datetime(df['published_at'], errors='coerce')
                
            # ê²°ì¸¡ì¹˜ ì œê±°
            time_df = df.dropna(subset=['published_at', 'final_sentiment_score'])
            
            if not time_df.empty:
                # ì¼ë³„ í†µê³„
                daily_stats = time_df.set_index('published_at')['final_sentiment_score'].resample('D').agg(['mean', 'std', 'count']).reset_index()
                daily_stats.columns = ['date', 'avg_sentiment_score', 'std_sentiment_score', 'article_count']
                
                # ì£¼ë³„ í†µê³„
                weekly_stats = time_df.set_index('published_at')['final_sentiment_score'].resample('W').agg(['mean', 'std', 'count']).reset_index()
                weekly_stats.columns = ['week', 'avg_sentiment_score', 'std_sentiment_score', 'article_count']
                
                # CSVë¡œ ì €ì¥
                daily_stats.to_csv(os.path.join(result_folder, f'daily_sentiment_stats_{timestamp}.csv'), index=False, encoding='utf-8-sig')
                weekly_stats.to_csv(os.path.join(result_folder, f'weekly_sentiment_stats_{timestamp}.csv'), index=False, encoding='utf-8-sig')
                print(f"ğŸ“Š ì‹œê°„ë³„ ê°ì • í†µê³„ ì €ì¥ ì™„ë£Œ.")
        
        # 2. ì •ì¹˜ ì„±í–¥ë³„ í†µê³„ ì €ì¥
        if 'bias' in df.columns:
            bias_stats = df.groupby('bias').agg({
                'final_sentiment_score': ['mean', 'std', 'count', 'min', 'max'],
                'sentiment_intensity_score': ['mean', 'std']
            }).reset_index()
            
            # ì»¬ëŸ¼ëª… ì¬êµ¬ì„±
            bias_stats.columns = ['_'.join(col).strip('_') for col in bias_stats.columns.values]
            
            # CSVë¡œ ì €ì¥
            bias_stats.to_csv(os.path.join(result_folder, f'political_bias_stats_{timestamp}.csv'), index=False, encoding='utf-8-sig')
            print(f"ğŸ“Š ì •ì¹˜ ì„±í–¥ë³„ ê°ì • í†µê³„ ì €ì¥ ì™„ë£Œ.")
        
        # 3. êµ­ê°€ë³„ í†µê³„ ì €ì¥
        if 'country' in df.columns:
            country_stats = df.groupby('country').agg({
                'final_sentiment_score': ['mean', 'std', 'count', 'min', 'max'],
                'sentiment_intensity_score': ['mean', 'std']
            }).reset_index()
            
            # ì»¬ëŸ¼ëª… ì¬êµ¬ì„±
            country_stats.columns = ['_'.join(col).strip('_') for col in country_stats.columns.values]
            
            # CSVë¡œ ì €ì¥
            country_stats.to_csv(os.path.join(result_folder, f'country_stats_{timestamp}.csv'), index=False, encoding='utf-8-sig')
            print(f"ğŸ“Š êµ­ê°€ë³„ ê°ì • í†µê³„ ì €ì¥ ì™„ë£Œ.")
        
        # 4. ì†ŒìŠ¤ë³„ í†µê³„ ì €ì¥
        if 'source' in df.columns:
            source_stats = df.groupby('source').agg({
                'final_sentiment_score': ['mean', 'std', 'count', 'min', 'max'],
                'sentiment_intensity_score': ['mean', 'std']
            }).reset_index()
            
            # ì»¬ëŸ¼ëª… ì¬êµ¬ì„±
            source_stats.columns = ['_'.join(col).strip('_') for col in source_stats.columns.values]
            
            # ìµœì†Œ 3ê°œ ì´ìƒì˜ ê¸°ì‚¬ê°€ ìˆëŠ” ì†ŒìŠ¤ë§Œ ìœ ì§€
            filtered_source_stats = source_stats[source_stats['final_sentiment_score_count'] >= 3]
            
            # CSVë¡œ ì €ì¥
            filtered_source_stats.to_csv(os.path.join(result_folder, f'news_source_stats_{timestamp}.csv'), index=False, encoding='utf-8-sig')
            print(f"ğŸ“Š ì‹ ë¬¸ì‚¬ë³„ ê°ì • í†µê³„ ì €ì¥ ì™„ë£Œ.")
        
        # 5. ì£¼ì œ-í‚¤ì›Œë“œë³„ í†µê³„ ì €ì¥
        if 'search_keyword' in df.columns:
            # í‚¤ì›Œë“œë³„ í†µê³„
            keyword_stats = df.groupby('search_keyword').agg({
                'final_sentiment_score': ['mean', 'std', 'count', 'min', 'max'],
                'sentiment_intensity_score': ['mean', 'std']
            }).reset_index()
            
            # ì»¬ëŸ¼ëª… ì¬êµ¬ì„±
            keyword_stats.columns = ['_'.join(col).strip('_') for col in keyword_stats.columns.values]
            
            # í‚¤ì›Œë“œ-ì£¼ì œ ë§¤í•‘
            keyword_to_topic = {}
            for topic, keywords in TOPIC_KEYWORDS.items():
                for keyword in keywords:
                    keyword_to_topic[keyword] = topic
            
            # ì£¼ì œ ì»¬ëŸ¼ ì¶”ê°€
            keyword_stats['topic'] = keyword_stats['search_keyword'].map(keyword_to_topic)
            
            # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
            cols = keyword_stats.columns.tolist()
            cols.insert(1, cols.pop(-1))  # 'topic' ì»¬ëŸ¼ì„ 'search_keyword' ë‹¤ìŒìœ¼ë¡œ ì´ë™
            keyword_stats = keyword_stats[cols]
            
            # CSVë¡œ ì €ì¥
            keyword_stats.to_csv(os.path.join(result_folder, f'keyword_stats_{timestamp}.csv'), index=False, encoding='utf-8-sig')
            print(f"ğŸ“Š í‚¤ì›Œë“œë³„ ê°ì • í†µê³„ ì €ì¥ ì™„ë£Œ.")
        
        # 6. êµ­ê°€-ì •ì¹˜ì„±í–¥ êµì°¨ í†µê³„ ì €ì¥
        if 'country' in df.columns and 'bias' in df.columns:
            cross_stats = df.groupby(['country', 'bias']).agg({
                'final_sentiment_score': ['mean', 'std', 'count'],
                'sentiment_intensity_score': ['mean', 'std']
            }).reset_index()
            
            # ì»¬ëŸ¼ëª… ì¬êµ¬ì„±
            cross_stats.columns = ['_'.join(col).strip('_') for col in cross_stats.columns.values]
            
            # CSVë¡œ ì €ì¥
            cross_stats.to_csv(os.path.join(result_folder, f'country_bias_cross_stats_{timestamp}.csv'), index=False, encoding='utf-8-sig')
            print(f"ğŸ“Š êµ­ê°€-ì •ì¹˜ì„±í–¥ êµì°¨ í†µê³„ ì €ì¥ ì™„ë£Œ.")
        
        # 7. ê°ì • ë¶„ì„ ë„êµ¬ ê°„ ë¹„êµ í†µê³„ ì €ì¥
        tool_cols = ['vader_score', 'google_score', 'huggingface_score', 'sentiment_intensity_score']
        available_tools = [col for col in tool_cols if col in df.columns]
        
        if len(available_tools) >= 2:
            # ìƒê´€ê´€ê³„ ê³„ì‚°
            tool_corr = df[available_tools].corr()
            
            # CSVë¡œ ì €ì¥
            tool_corr.to_csv(os.path.join(result_folder, f'sentiment_tools_correlation_{timestamp}.csv'), encoding='utf-8-sig')
            print(f"ğŸ“Š ê°ì • ë¶„ì„ ë„êµ¬ ê°„ ìƒê´€ê´€ê³„ í†µê³„ ì €ì¥ ì™„ë£Œ.")
            
            # ë„êµ¬ë³„ í‰ê·  ë° í‘œì¤€í¸ì°¨
            tool_stats = {tool: {'mean': df[tool].mean(), 'std': df[tool].std()} for tool in available_tools}
            tool_stats_df = pd.DataFrame(tool_stats).T
            tool_stats_df.index.name = 'sentiment_tool'
            tool_stats_df.reset_index(inplace=True)
            
            # CSVë¡œ ì €ì¥
            tool_stats_df.to_csv(os.path.join(result_folder, f'sentiment_tools_stats_{timestamp}.csv'), index=False, encoding='utf-8-sig')
            print(f"ğŸ“Š ê°ì • ë¶„ì„ ë„êµ¬ë³„ í†µê³„ ì €ì¥ ì™„ë£Œ.")
            
        print(f"âœ… ëª¨ë“  ì°¨ì›ì˜ í†µê³„ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âš ï¸ ì¢…í•© í†µê³„ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

def plot_tool_correlation(df, result_folder, timestamp):
    """ê°ì • ë¶„ì„ ë„êµ¬ ê°„ ìƒê´€ê´€ê³„ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”"""
    try:
        # ì‚¬ìš© ê°€ëŠ¥í•œ ê°ì • ë¶„ì„ ë„êµ¬ í™•ì¸
        available_tools = []
        tool_labels = []
        
        if 'vader_score' in df.columns:
            available_tools.append('vader_score')
            tool_labels.append('VADER')
        if 'google_score' in df.columns:
            available_tools.append('google_score')
            tool_labels.append('Google NLP')
        if 'huggingface_score' in df.columns:
            available_tools.append('huggingface_score')
            tool_labels.append('HuggingFace')
        if 'sentiment_intensity_score' in df.columns:
            available_tools.append('sentiment_intensity_score')
            tool_labels.append('SentiStrength')
            
        if len(available_tools) < 2:
            print("âš ï¸ ë„êµ¬ ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        correlation = df[available_tools].corr()
        
        # íˆíŠ¸ë§µ ì‹œê°í™”
        plt.figure(figsize=(10, 8))
        sns.heatmap(correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1,
                   xticklabels=tool_labels, yticklabels=tool_labels)
        plt.title('Correlation Between Sentiment Analysis Tools')
        plt.tight_layout()
        plt.savefig(os.path.join(result_folder, f'tool_correlation_{timestamp}.png'))
        print(f"ğŸ“Š ê°ì • ë¶„ì„ ë„êµ¬ ê°„ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ì €ì¥ ì™„ë£Œ.")
            
    except Exception as e:
        print(f"âš ï¸ ë„êµ¬ ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def validate_dataframe(df, required_columns, min_rows=5, purpose="ë¶„ì„"):
    """ë°ì´í„°í”„ë ˆì„ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ ê²€ì¦"""
    
    # ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°
    if df is None or len(df) == 0:
        print(f"âš ï¸ ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. {purpose}ì„(ë¥¼) ê±´ë„ˆëœë‹ˆë‹¤.")
        return False
        
    # ìµœì†Œ í–‰ ìˆ˜ í™•ì¸
    if len(df) < min_rows:
        print(f"âš ï¸ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (í•„ìš”: {min_rows}ê°œ, í˜„ì¬: {len(df)}ê°œ) {purpose}ì„(ë¥¼) ê±´ë„ˆëœë‹ˆë‹¤.")
        return False
    
    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        # ë‚ ì§œ ì»¬ëŸ¼ì— ëŒ€í•œ ëŒ€ì²´ í™•ì¸ ('published_at' ë˜ëŠ” 'date' ì¤‘ í•˜ë‚˜ í•„ìš”)
        if 'published_at' in missing_columns and 'date' in df.columns:
            # date ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ published_atìœ¼ë¡œ ë³µì‚¬
            df['published_at'] = df['date']
            missing_columns.remove('published_at')
        
        if 'date' in missing_columns and 'published_at' in df.columns:
            # published_at ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ dateë¡œ ë³µì‚¬
            df['date'] = df['published_at']
            missing_columns.remove('date')
            
        # ì—¬ì „íˆ ëˆ„ë½ëœ ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥
        if missing_columns:
            print(f"âš ï¸ í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}. {purpose}ì„(ë¥¼) ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
        
    return True

def preprocess_time_series_data(df):
    """ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜"""
    # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
    date_columns = ['published_at', 'date']
    date_column = None
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°
    for col in date_columns:
        if col in df.columns:
            date_column = col
            break
    
    if date_column is None:
        # ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° í˜„ì¬ ë‚ ì§œë¡œ ìƒì„±
        print("âš ï¸ ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ë‚ ì§œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        df['published_at'] = datetime.datetime.now()
        date_column = 'published_at'
    
    # ë‚ ì§œ í˜•ì‹ ë³€í™˜
    if df[date_column].dtype == 'object':
        df[date_column] = pd.to_datetime(df[date_column], errors='coerce')
    
    # ê²°ì¸¡ ë‚ ì§œ ì²˜ë¦¬
    df = df.dropna(subset=[date_column])
    
    # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬
    df = df.sort_values(date_column)
    
    # ë‹¤ë¥¸ ë‚ ì§œ ì»¬ëŸ¼ì´ í•„ìš”í•˜ë©´ ë³µì‚¬
    for col in date_columns:
        if col != date_column and col not in df.columns:
            df[col] = df[date_column]
    
    return df

# ë‚ ì§œ ì²˜ë¦¬ ê°œì„  í•¨ìˆ˜
def ensure_datetime_format(date_str):
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
    if isinstance(date_str, pd.Timestamp) or isinstance(date_str, datetime.datetime):
        return date_str
    
    try:
        return pd.to_datetime(date_str)
    except:
        # ë‹¤ì–‘í•œ ë‚ ì§œ í¬ë§· ì‹œë„
        formats = [
            '%Y-%m-%d', '%Y/%m/%d', '%d-%m-%Y', '%d/%m/%Y',
            '%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S'
        ]
        
        for fmt in formats:
            try:
                return datetime.datetime.strptime(date_str, fmt)
            except:
                continue
                
        # ëª¨ë“  í¬ë§· ì‹œë„ ì‹¤íŒ¨ì‹œ
        print(f"ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {date_str}")
        return None

def safe_visualization_wrapper(func):
    """ì‹œê°í™” í•¨ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•˜ëŠ” ë°ì½”ë ˆì´í„°"""
    def wrapper(*args, **kwargs):
        try:
            result = func(*args, **kwargs)
            plt.close('all')  # ëª¨ë“  ì—´ë¦° ê·¸ë˜í”„ ë‹«ê¸°
            return result
        except Exception as e:
            print(f"âš ï¸ {func.__name__} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            if plt.gcf().number:
                plt.close('all')  # ì—´ë¦° figure ë‹«ê¸°
    return wrapper

def save_detailed_statistics(df, result_folder, timestamp):
    """ëª¨ë“  ì°¨ì›ì˜ ìƒì„¸ í†µê³„ë¥¼ ì €ì¥í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜"""
    
    # 1. ì„±í–¥ë³„ í†µê³„
    if 'bias' in df.columns:
        bias_stats = df.groupby('bias').agg({
            'final_sentiment_score': ['count', 'mean', 'median', 'var', 'std', 'min', 'max'],
            'sentiment_intensity_score': ['mean', 'var', 'std']
        }).reset_index()
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        bias_stats.columns = ['_'.join(col).strip('_') for col in bias_stats.columns.values]
        
        # ì €ì¥
        bias_stats.to_csv(f"{result_folder}/bias_statistics_{timestamp}.csv", 
                        index=False, encoding="utf-8-sig")
        print(f"ğŸ“Š ì •ì¹˜ ì„±í–¥ë³„ í†µê³„ ì €ì¥ ì™„ë£Œ")
    
    # 2. ë„êµ¬ë³„ ì ìˆ˜ í†µê³„
    tool_cols = ['vader_score', 'google_score', 'huggingface_score', 'sentiment_intensity_score']
    available_tools = [col for col in tool_cols if col in df.columns]
    
    if available_tools:
        tool_stats = pd.DataFrame()
        for tool in available_tools:
            stats = df[tool].agg(['count', 'mean', 'median', 'var', 'std']).reset_index()
            stats.columns = ['statistic', tool]
            if tool_stats.empty:
                tool_stats = stats
            else:
                tool_stats = pd.merge(tool_stats, stats, on='statistic')
        
        tool_stats.to_csv(f"{result_folder}/tools_statistics_{timestamp}.csv", 
                       index=False, encoding="utf-8-sig")
        print(f"ğŸ“Š ê°ì • ë¶„ì„ ë„êµ¬ë³„ í†µê³„ ì €ì¥ ì™„ë£Œ")
    
    # 3. í‚¤ì›Œë“œë³„ í†µê³„
    if 'search_keyword' in df.columns:
        keyword_stats = df.groupby('search_keyword').agg({
            'final_sentiment_score': ['count', 'mean', 'var', 'std'],
            'sentiment_intensity_score': ['mean', 'var', 'std']
        }).reset_index()
        
        keyword_stats.columns = ['_'.join(col).strip('_') for col in keyword_stats.columns.values]
        keyword_stats.to_csv(f"{result_folder}/keyword_statistics_{timestamp}.csv", 
                          index=False, encoding="utf-8-sig")
        print(f"ğŸ“Š í‚¤ì›Œë“œë³„ í†µê³„ ì €ì¥ ì™„ë£Œ")

def check_date_distribution(df):
    """ë°ì´í„°ì˜ ë‚ ì§œ ë¶„í¬ë¥¼ í™•ì¸í•˜ëŠ” í•¨ìˆ˜"""
    if 'published_at' in df.columns:
        # ë‚ ì§œ ë³€í™˜
        df['published_at'] = pd.to_datetime(df['published_at'], errors='coerce')
        date_count = df['published_at'].dt.date.value_counts().sort_index()
        
        print("\n===== ë‚ ì§œë³„ ê¸°ì‚¬ ìˆ˜ =====")
        for date, count in date_count.items():
            print(f"{date}: {count}ê°œ ê¸°ì‚¬")
        
        unique_dates = len(date_count)
        print(f"\nì´ {unique_dates}ê°œì˜ ê³ ìœ  ë‚ ì§œê°€ ìˆìŠµë‹ˆë‹¤.")
        
        if unique_dates < 3:
            print("âš ï¸ ê²½ê³ : ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•œ ì¶©ë¶„í•œ ë‚ ì§œ ë¶„í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë³€í™˜ ì‹¤íŒ¨í•œ ë‚ ì§œ í™•ì¸
        null_dates = df['published_at'].isnull().sum()
        if null_dates > 0:
            print(f"âš ï¸ ê²½ê³ : {null_dates}ê°œì˜ ë‚ ì§œë¥¼ ë³€í™˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
    
